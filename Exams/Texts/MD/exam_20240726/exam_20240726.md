# Machine Learning – July 26, 2024

| Matricola | Last Name | First Name |
|-----------|-----------|------------|

## Notes

- 1. No books, slides, written notes are allowed during the exam.
- 2. Answers must be explicitly marked with the question they refer to (e.g., 2.1 for question 1 of exercise 2). Cumulative answers which refer to more questions will be evaluated as answering one question only.

Time limit: 2 hours.

#### EXERCISE 1

- 1. Describe with pseudo-code the K-Fold Cross Validation method to estimate the accuracy of a learning algorithm L on a dataset D.
- 2. Describe how the method can be extended to compare two different learning algorithms LA, LB.

# EXERCISE 2

- 1. Describe the perceptron model for classification and its training rule.
- 2. Draw a graphical representation of a linearly separable 2D data set for binary classification and provide a qualitative graphical example of a possible evolution of perceptron training with two different values of learning rate: one very small, one large. Draw 4 images for each parameter (very small and large), in each image show the same data set and the current model computed from the algorithm during its steps. Start with the same initial configuration of the model that is not a solution of the classification problem. Show different evolution from this situation according to the different values of the learning rate. The last image of the sequence should show a situation corresponding to a termination condition.

#### EXERCISE 3

Consider a setting where the input space I is the set of finite strings over the characters a, b, c. Notice that input strings can be of different length. Given the dataset on the table on the right:

- 1. Identify the learning problem at hand, in particular the form of the target function.
- 2. Define a suitable kernelized linear model for this problem.
- 3. Define a kernel function suitable to measure the similarity of data sample for this problem.

| x          | t  |
|------------|----|
| b          | 1  |
| a          | 2  |
| ab         | 2  |
| caba       | 4  |
| abca       | 4  |
| aabba      | 8  |
| aaa        | 8  |
| babaa      | 8  |
| abcaaca    | 16 |
| bcaaaca    | 16 |
| abcbabacca | 16 |

## EXERCISE 4

Consider the k-armed bandit problem (also known as One-state MDP) with stochastic Gaussian behavior and unknown reward functions.

- 1. Formally describe the model, provide mathematical definitions and explanations of all the elements of the model.
- 2. Formally describe the problem and the solution concept (i.e., what is the problem to be solved with the model described in the previous point), provide mathematical definitions and explanations of all the elements of the problem.
- 3. Formally describe the Reinforcement Learning algorithm to compute the optimal policy for this problem, provide explanations of all the terms of the algorithm.

#### EXERCISE 5

Consider a layer of a CNN with input feature maps of size 96 × 96 × 10.

- 1. Provide the design of the layer (including all details about kernel, pooling and activation functions) to obtain output feature maps of size 30 × 30 × 25. Note: kernel and pooling size should be small (less than 10 × 10).
- 2. How many trainable parameters are produced by this layer?

#### EXERCISE 6

Given an unsupervised dataset D = {xn}

- 1. Define the Gassian Mixture Model (GMM) and describe the parameters of the model.
- 2. Draw an example of a 2D data set (i.e., D ⊂ ℜ<sup>2</sup> ) generated by a GMM with K = 3 with significant difference of all the parameters of the model.
- 3. Draw a possible solution obtained by the Expectation-Maximization algorithm algorithm on the data set shown in point 2.
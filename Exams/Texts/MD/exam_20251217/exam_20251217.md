# Machine Learning – December 17, 2025

| Matricola | Last Name | First Name |
|-----------|-----------|------------|
|           |           |            |
|           |           |            |
|           |           |            |

- 1. No books, slides, written notes are allowed during the exam.
- 2. Answers must be explicitly marked with the question they refer to (e.g., 2.1 for question 1 of exercise 2). Cumulative answers which refer to more questions will be evaluated as answering one question only.

Time limit: 2 hours.

# EXERCISE 1

Consider a binary classification problem X → {T, F}, with X = {T, F} 3 , i.e. (x1, x2, x3) ∈ X and x<sup>i</sup> ∈ {T, F}, and the dataset D = {⟨(F, F, F), F⟩,⟨(F, T, T), T⟩,⟨(T, T, F), T⟩,⟨(T, F, T), T⟩}. Consider the two hypothesis h<sup>1</sup> = (x<sup>1</sup> ∧ ¬x<sup>2</sup> ∧ x3) ∨ x<sup>2</sup> and h<sup>2</sup> = (¬x<sup>1</sup> ∧ x<sup>2</sup> ∧ x3) ∨ x1.

- 1. Determine whether h<sup>1</sup> and h<sup>2</sup> are consistent with D, showing all the passages needed to answer.
- 2. Assuming the likelihood probabilities P(D|h1) = 0.6 and P(D|h2) = 0.8 and the prior probabilities P(h1) = 0.2 and P(h2) = 0.1, determine the higher a poteriori hypothesis between h<sup>1</sup> and h2.

# EXERCISE 2

Consider the problem of estimating the function f : ℜ <sup>3</sup> → ℜ, with dataset D = {(x T 1 , t1), . . . ,(x T <sup>N</sup> , t<sup>N</sup> )} and using a feed-forward network.

- 1. Explain what is a suitable choice for the loss function used for training the network and write the corresponding mathematical expression.
- 2. Assuming that the gradients of the loss with respect to the parameters are available, describe an algorithm for training the parameters of the network. What are the hyper-parameters of the training algorithm (if any)?

#### EXERCISE 3

- 1. Describe the principle of maximum margin used by SVM classifiers through its formal mathematical definition.
- 2. Draw a linearly separable dataset for 2D binary classification. Draw a possible solution obtained by SVM and highights the margin and the support vectors.
- 3. Discuss why the maximum margin solution is preferred for the classification problem.

# EXERCISE 4

Consider a dataset D for the binary classification problem f : ℜ 3 7→ {Y, N}.

- 1. Describe a probabilistic generative model for such a classification problem, assuming Gaussian distributions.
- 2. Identify the parameters of the model and determine the size of the model (i.e., the number of independent parameters).

### EXERCISE 5

Consider the following dataset, containing the samples of a function f:

| x1  | x2  | f   |
|-----|-----|-----|
| 0.0 | 0.0 | 1.0 |
| 0.6 | 0.3 | 2.2 |
| 1.2 | 0.4 | 3.0 |
| 1.5 | 0.5 | 3.5 |

- 1. Based on the available data, select a reasonable model for learning f, explicitly indicating its parameters.
- 2. Show an optimal and a non-optimal solution, explicitly indicating, for each of them, the corresponding value of the loss function.

#### EXERCISE 6

- 1. Describe the concept of bagging in the definition of an ensemble model. Describe precisely the training procedure for such a model and the final formula used for prediction.
- 2. Discuss the difference between bagging and voting, highlighting in particular the use of different types of models.